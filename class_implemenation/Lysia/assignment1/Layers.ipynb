{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the cell content back into a file add the cell-magic %%writefile filename.py at the beginning of the cell and run it. Beware that if a file with the same name already exists it will be silently overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "    then transform it to an output vector of dimension M.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    - w: A numpy array of weights, of shape (D, M)\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the affine forward pass. Store the result in out. You     #\n",
    "    # will need to reshape the input into rows.                                 #\n",
    "    #############################################################################\n",
    "    \n",
    "    N = x.shape[0]\n",
    "    D = np.prod(x.shape[1:])\n",
    "    M = len(b)\n",
    "    \n",
    "    x_p = np.reshape(x, (N, D))\n",
    "    out = x_p.dot(w) + b\n",
    "    \n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    cache = (x, w, b)\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test case dim:\n",
    "# N: 10; D: 6; M: 5\n",
    "# dout: N x M = 10 x 5\n",
    "def affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an affine layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, M)\n",
    "    - cache: Tuple of:\n",
    "    - x: Input data, of shape (N, d_1, ... d_k)\n",
    "    - w: Weights, of shape (D, M)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "    - dw: Gradient with respect to w, of shape (D, M)\n",
    "    - db: Gradient with respect to b, of shape (M,)\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "    dx, dw, db = None, None, None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the affine backward pass.                                 #\n",
    "    #############################################################################\n",
    "    N = x.shape[0]\n",
    "    D = np.prod(x.shape[1:])\n",
    "    M = len(b)\n",
    "    \n",
    "    dx = dout.dot(w.T).reshape(x.shape)\n",
    "    dw = x.reshape(N, D).T.dot(dout)\n",
    "    db = np.sum(dout, axis = 0)\n",
    "\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# x: output from affine_forward, of shape (N, M/H)\n",
    "# out: output is N x H\n",
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - x: Inputs, of any shape\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output, of the same shape as x\n",
    "    - cache: x\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the ReLU forward pass.                                    #\n",
    "    #############################################################################\n",
    "    \n",
    "    relu = lambda x: max(0.0, x)\n",
    "    vfunc = np.vectorize(relu)\n",
    "    tmp = vfunc(x.reshape(np.prod(x.shape)))\n",
    "    out = tmp.reshape(x.shape)\n",
    "    \n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    cache = x\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: Input x, of same shape as dout\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    dx, x = None, cache\n",
    "    #############################################################################\n",
    "    # TODO: Implement the ReLU backward pass.                                   #\n",
    "    #############################################################################\n",
    "    \n",
    "    x_flat = x.reshape(np.prod(x.shape))\n",
    "    dout_flat = dout.reshape(np.prod(dout.shape))\n",
    "    dx_flat = np.array([d_f if x_f > 0 else 0.0 for x_f, d_f in zip(x_flat, dout_flat) ])\n",
    "    dx = dx_flat.reshape(x.shape)\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_backward function:\n",
      "dx error:  3.27561905336e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "# print(x)\n",
    "# print(\"sdfasd\")\n",
    "# print(dx_num)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-12\n",
    "print 'Testing relu_backward function:'\n",
    "print 'dx error: ', rel_error(dx_num, dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./deeplearning/layers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./deeplearning/layers.py\n",
    "# %load ./deeplearning/layers.py\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def affine_forward(x, w, b):\n",
    "  \"\"\"\n",
    "  Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "  The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "  examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "  reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "  then transform it to an output vector of dimension M.\n",
    "\n",
    "  Inputs:\n",
    "  - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "  - w: A numpy array of weights, of shape (D, M)\n",
    "  - b: A numpy array of biases, of shape (M,)\n",
    "  \n",
    "  Returns a tuple of:\n",
    "  - out: output, of shape (N, M)\n",
    "  - cache: (x, w, b)\n",
    "  \"\"\"\n",
    "  out = None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the affine forward pass. Store the result in out. You     #\n",
    "  # will need to reshape the input into rows.                                 #\n",
    "  #############################################################################\n",
    "  N = x.shape[0]\n",
    "  D = np.prod(x.shape[1:])\n",
    "  M = len(b)\n",
    "  x_p = np.reshape(x, (N, D))\n",
    "  out = x_p.dot(w) + b\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  cache = (x, w, b)\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def affine_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Computes the backward pass for an affine layer.\n",
    "\n",
    "  Inputs:\n",
    "  - dout: Upstream derivative, of shape (N, M)\n",
    "  - cache: Tuple of:\n",
    "    - x: Input data, of shape (N, d_1, ... d_k)\n",
    "    - w: Weights, of shape (D, M)\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "  - dw: Gradient with respect to w, of shape (D, M)\n",
    "  - db: Gradient with respect to b, of shape (M,)\n",
    "  \"\"\"\n",
    "  x, w, b = cache\n",
    "  dx, dw, db = None, None, None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the affine backward pass.                                 #\n",
    "  #############################################################################\n",
    "  N = x.shape[0]\n",
    "  D = np.prod(x.shape[1:])\n",
    "  M = len(b)\n",
    "    \n",
    "  dx = dout.dot(w.T).reshape(x.shape)\n",
    "  dw = x.reshape(N, D).T.dot(dout)\n",
    "  db = np.sum(dout, axis = 0)\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  return dx, dw, db\n",
    "\n",
    "\n",
    "def relu_forward(x):\n",
    "  \"\"\"\n",
    "  Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "  Input:\n",
    "  - x: Inputs, of any shape\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - out: Output, of the same shape as x\n",
    "  - cache: x\n",
    "  \"\"\"\n",
    "  out = None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the ReLU forward pass.                                    #\n",
    "  #############################################################################\n",
    "  relu = lambda x: max(0.0, x)\n",
    "  vfunc = np.vectorize(relu)\n",
    "  tmp = vfunc(x.reshape(np.prod(x.shape)))\n",
    "  out = tmp.reshape(x.shape)\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  cache = x\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def relu_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "  Input:\n",
    "  - dout: Upstream derivatives, of any shape\n",
    "  - cache: Input x, of same shape as dout\n",
    "\n",
    "  Returns:\n",
    "  - dx: Gradient with respect to x\n",
    "  \"\"\"\n",
    "  dx, x = None, cache\n",
    "  #############################################################################\n",
    "  # TODO: Implement the ReLU backward pass.                                   #\n",
    "  #############################################################################\n",
    "  x_flat = x.reshape(np.prod(x.shape))\n",
    "  dout_flat = dout.reshape(np.prod(dout.shape))\n",
    "  dx_flat = np.array([d_f if x_f > 0 else 0.0 for x_f, d_f in zip(x_flat, dout_flat) ])\n",
    "  dx = dx_flat.reshape(x.shape)\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  return dx\n",
    "\n",
    "\n",
    "def batchnorm_forward(x, gamma, beta, bn_param):\n",
    "  \"\"\"\n",
    "  Forward pass for batch normalization.\n",
    "  \n",
    "  During training the sample mean and (uncorrected) sample variance are\n",
    "  computed from minibatch statistics and used to normalize the incoming data.\n",
    "  During training we also keep an exponentially decaying running mean of the mean\n",
    "  and variance of each feature, and these averages are used to normalize data\n",
    "  at test-time.\n",
    "\n",
    "  At each timestep we update the running averages for mean and variance using\n",
    "  an exponential decay based on the momentum parameter:\n",
    "\n",
    "  running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n",
    "  running_var = momentum * running_var + (1 - momentum) * sample_var\n",
    "\n",
    "  Note that the batch normalization paper suggests a different test-time\n",
    "  behavior: they compute sample mean and variance for each feature using a\n",
    "  large number of training images rather than using a running average. For\n",
    "  this implementation we have chosen to use running averages instead since\n",
    "  they do not require an additional estimation step; the torch7 implementation\n",
    "  of batch normalization also uses running averages.\n",
    "\n",
    "  Input:\n",
    "  - x: Data of shape (N, D)\n",
    "  - gamma: Scale parameter of shape (D,)\n",
    "  - beta: Shift paremeter of shape (D,)\n",
    "  - bn_param: Dictionary with the following keys:\n",
    "    - mode: 'train' or 'test'; required\n",
    "    - eps: Constant for numeric stability\n",
    "    - momentum: Constant for running mean / variance.\n",
    "    - running_mean: Array of shape (D,) giving running mean of features\n",
    "    - running_var Array of shape (D,) giving running variance of features\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - out: of shape (N, D)\n",
    "  - cache: A tuple of values needed in the backward pass\n",
    "  \"\"\"\n",
    "  mode = bn_param['mode']\n",
    "  eps = bn_param.get('eps', 1e-5)\n",
    "  momentum = bn_param.get('momentum', 0.9)\n",
    "\n",
    "  N, D = x.shape\n",
    "  running_mean = bn_param.get('running_mean', np.zeros(D, dtype=x.dtype))\n",
    "  running_var = bn_param.get('running_var', np.zeros(D, dtype=x.dtype))\n",
    "\n",
    "  out, cache = None, None\n",
    "  if mode == 'train':\n",
    "    #############################################################################\n",
    "    # TODO: Implement the training-time forward pass for batch normalization.   #\n",
    "    # Use minibatch statistics to compute the mean and variance, use these      #\n",
    "    # statistics to normalize the incoming data, and scale and shift the        #\n",
    "    # normalized data using gamma and beta.                                     #\n",
    "    #                                                                           #\n",
    "    # You should store the output in the variable out. Any intermediates that   #\n",
    "    # you need for the backward pass should be stored in the cache variable.    #\n",
    "    #                                                                           #\n",
    "    # You should also use your computed sample mean and variance together with  #\n",
    "    # the momentum variable to update the running mean and running variance,    #\n",
    "    # storing your result in the running_mean and running_var variables.        #\n",
    "    #############################################################################\n",
    "    pass\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "  elif mode == 'test':\n",
    "    #############################################################################\n",
    "    # TODO: Implement the test-time forward pass for batch normalization. Use   #\n",
    "    # the running mean and variance to normalize the incoming data, then scale  #\n",
    "    # and shift the normalized data using gamma and beta. Store the result in   #\n",
    "    # the out variable.                                                         #\n",
    "    #############################################################################\n",
    "    pass\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "  else:\n",
    "    raise ValueError('Invalid forward batchnorm mode \"%s\"' % mode)\n",
    "\n",
    "  # Store the updated running means back into bn_param\n",
    "  bn_param['running_mean'] = running_mean\n",
    "  bn_param['running_var'] = running_var\n",
    "\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def batchnorm_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Backward pass for batch normalization.\n",
    "  \n",
    "  For this implementation, you should write out a computation graph for\n",
    "  batch normalization on paper and propagate gradients backward through\n",
    "  intermediate nodes.\n",
    "  \n",
    "  Inputs:\n",
    "  - dout: Upstream derivatives, of shape (N, D)\n",
    "  - cache: Variable of intermediates from batchnorm_forward.\n",
    "  \n",
    "  Returns a tuple of:\n",
    "  - dx: Gradient with respect to inputs x, of shape (N, D)\n",
    "  - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)\n",
    "  - dbeta: Gradient with respect to shift parameter beta, of shape (D,)\n",
    "  \"\"\"\n",
    "  dx, dgamma, dbeta = None, None, None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the backward pass for batch normalization. Store the      #\n",
    "  # results in the dx, dgamma, and dbeta variables.                           #\n",
    "  #############################################################################\n",
    "  pass\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "\n",
    "  return dx, dgamma, dbeta\n",
    "\n",
    "\n",
    "def batchnorm_backward_alt(dout, cache):\n",
    "  \"\"\"\n",
    "  Alternative backward pass for batch normalization.\n",
    "  \n",
    "  For this implementation you should work out the derivatives for the batch\n",
    "  normalizaton backward pass on paper and simplify as much as possible. You\n",
    "  should be able to derive a simple expression for the backward pass.\n",
    "  \n",
    "  Note: This implementation should expect to receive the same cache variable\n",
    "  as batchnorm_backward, but might not use all of the values in the cache.\n",
    "  \n",
    "  Inputs / outputs: Same as batchnorm_backward\n",
    "  \"\"\"\n",
    "  dx, dgamma, dbeta = None, None, None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the backward pass for batch normalization. Store the      #\n",
    "  # results in the dx, dgamma, and dbeta variables.                           #\n",
    "  #                                                                           #\n",
    "  # After computing the gradient with respect to the centered inputs, you     #\n",
    "  # should be able to compute gradients with respect to the inputs in a       #\n",
    "  # single statement; our implementation fits on a single 80-character line.  #\n",
    "  #############################################################################\n",
    "  pass\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  \n",
    "  return dx, dgamma, dbeta\n",
    "\n",
    "\n",
    "def dropout_forward(x, dropout_param):\n",
    "  \"\"\"\n",
    "  Performs the forward pass for (inverted) dropout.\n",
    "\n",
    "  Inputs:\n",
    "  - x: Input data, of any shape\n",
    "  - dropout_param: A dictionary with the following keys:\n",
    "    - p: Dropout parameter. We drop each neuron output with probability p.\n",
    "    - mode: 'test' or 'train'. If the mode is train, then perform dropout;\n",
    "      if the mode is test, then just return the input.\n",
    "    - seed: Seed for the random number generator. Passing seed makes this\n",
    "      function deterministic, which is needed for gradient checking but not in\n",
    "      real networks.\n",
    "\n",
    "  Outputs:\n",
    "  - out: Array of the same shape as x.\n",
    "  - cache: A tuple (dropout_param, mask). In training mode, mask is the dropout\n",
    "    mask that was used to multiply the input; in test mode, mask is None.\n",
    "  \"\"\"\n",
    "  p, mode = dropout_param['p'], dropout_param['mode']\n",
    "  if 'seed' in dropout_param:\n",
    "    np.random.seed(dropout_param['seed'])\n",
    "\n",
    "  mask = None\n",
    "  out = None\n",
    "\n",
    "  if mode == 'train':\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the training phase forward pass for inverted dropout.   #\n",
    "    # Store the dropout mask in the mask variable.                            #\n",
    "    ###########################################################################\n",
    "    pass\n",
    "    ###########################################################################\n",
    "    #                            END OF YOUR CODE                             #\n",
    "    ###########################################################################\n",
    "  elif mode == 'test':\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the test phase forward pass for inverted dropout.       #\n",
    "    ###########################################################################\n",
    "    pass\n",
    "    ###########################################################################\n",
    "    #                            END OF YOUR CODE                             #\n",
    "    ###########################################################################\n",
    "\n",
    "  cache = (dropout_param, mask)\n",
    "  out = out.astype(x.dtype, copy=False)\n",
    "\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def dropout_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Perform the backward pass for (inverted) dropout.\n",
    "\n",
    "  Inputs:\n",
    "  - dout: Upstream derivatives, of any shape\n",
    "  - cache: (dropout_param, mask) from dropout_forward.\n",
    "  \"\"\"\n",
    "  dropout_param, mask = cache\n",
    "  mode = dropout_param['mode']\n",
    "  \n",
    "  dx = None\n",
    "  if mode == 'train':\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the training phase backward pass for inverted dropout.  #\n",
    "    ###########################################################################\n",
    "    pass\n",
    "    ###########################################################################\n",
    "    #                            END OF YOUR CODE                             #\n",
    "    ###########################################################################\n",
    "  elif mode == 'test':\n",
    "    dx = dout\n",
    "  return dx\n",
    "\n",
    "\n",
    "def conv_forward_naive(x, w, b, conv_param):\n",
    "  \"\"\"\n",
    "  A naive implementation of the forward pass for a convolutional layer.\n",
    "\n",
    "  The input consists of N data points, each with C channels, height H and width\n",
    "  W. We convolve each input with F different filters, where each filter spans\n",
    "  all C channels and has height HH and width HH.\n",
    "\n",
    "  Input:\n",
    "  - x: Input data of shape (N, C, H, W)\n",
    "  - w: Filter weights of shape (F, C, HH, WW)\n",
    "  - b: Biases, of shape (F,)\n",
    "  - conv_param: A dictionary with the following keys:\n",
    "    - 'stride': The number of pixels between adjacent receptive fields in the\n",
    "      horizontal and vertical directions.\n",
    "    - 'pad': The number of pixels that will be used to zero-pad the input.\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - out: Output data, of shape (N, F, H', W') where H' and W' are given by\n",
    "    H' = 1 + (H + 2 * pad - HH) / stride\n",
    "    W' = 1 + (W + 2 * pad - WW) / stride\n",
    "  - cache: (x, w, b, conv_param)\n",
    "  \"\"\"\n",
    "  out = None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the convolutional forward pass.                           #\n",
    "  # Hint: you can use the function np.pad for padding.                        #\n",
    "  #############################################################################\n",
    "  pass\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  cache = (x, w, b, conv_param)\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def conv_backward_naive(dout, cache):\n",
    "  \"\"\"\n",
    "  A naive implementation of the backward pass for a convolutional layer.\n",
    "\n",
    "  Inputs:\n",
    "  - dout: Upstream derivatives.\n",
    "  - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - dx: Gradient with respect to x\n",
    "  - dw: Gradient with respect to w\n",
    "  - db: Gradient with respect to b\n",
    "  \"\"\"\n",
    "  dx, dw, db = None, None, None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the convolutional backward pass.                          #\n",
    "  #############################################################################\n",
    "  pass\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  return dx, dw, db\n",
    "\n",
    "\n",
    "def max_pool_forward_naive(x, pool_param):\n",
    "  \"\"\"\n",
    "  A naive implementation of the forward pass for a max pooling layer.\n",
    "\n",
    "  Inputs:\n",
    "  - x: Input data, of shape (N, C, H, W)\n",
    "  - pool_param: dictionary with the following keys:\n",
    "    - 'pool_height': The height of each pooling region\n",
    "    - 'pool_width': The width of each pooling region\n",
    "    - 'stride': The distance between adjacent pooling regions\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - out: Output data\n",
    "  - cache: (x, pool_param)\n",
    "  \"\"\"\n",
    "  out = None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the max pooling forward pass                              #\n",
    "  #############################################################################\n",
    "  pass\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  cache = (x, pool_param)\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def max_pool_backward_naive(dout, cache):\n",
    "  \"\"\"\n",
    "  A naive implementation of the backward pass for a max pooling layer.\n",
    "\n",
    "  Inputs:\n",
    "  - dout: Upstream derivatives\n",
    "  - cache: A tuple of (x, pool_param) as in the forward pass.\n",
    "\n",
    "  Returns:\n",
    "  - dx: Gradient with respect to x\n",
    "  \"\"\"\n",
    "  dx = None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the max pooling backward pass                             #\n",
    "  #############################################################################\n",
    "  pass\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  return dx\n",
    "\n",
    "\n",
    "def spatial_batchnorm_forward(x, gamma, beta, bn_param):\n",
    "  \"\"\"\n",
    "  Computes the forward pass for spatial batch normalization.\n",
    "  \n",
    "  Inputs:\n",
    "  - x: Input data of shape (N, C, H, W)\n",
    "  - gamma: Scale parameter, of shape (C,)\n",
    "  - beta: Shift parameter, of shape (C,)\n",
    "  - bn_param: Dictionary with the following keys:\n",
    "    - mode: 'train' or 'test'; required\n",
    "    - eps: Constant for numeric stability\n",
    "    - momentum: Constant for running mean / variance. momentum=0 means that\n",
    "      old information is discarded completely at every time step, while\n",
    "      momentum=1 means that new information is never incorporated. The\n",
    "      default of momentum=0.9 should work well in most situations.\n",
    "    - running_mean: Array of shape (D,) giving running mean of features\n",
    "    - running_var Array of shape (D,) giving running variance of features\n",
    "    \n",
    "  Returns a tuple of:\n",
    "  - out: Output data, of shape (N, C, H, W)\n",
    "  - cache: Values needed for the backward pass\n",
    "  \"\"\"\n",
    "  out, cache = None, None\n",
    "\n",
    "  #############################################################################\n",
    "  # TODO: Implement the forward pass for spatial batch normalization.         #\n",
    "  #                                                                           #\n",
    "  # HINT: You can implement spatial batch normalization using the vanilla     #\n",
    "  # version of batch normalization defined above. Your implementation should  #\n",
    "  # be very short; ours is less than five lines.                              #\n",
    "  #############################################################################\n",
    "  pass\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def spatial_batchnorm_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Computes the backward pass for spatial batch normalization.\n",
    "  \n",
    "  Inputs:\n",
    "  - dout: Upstream derivatives, of shape (N, C, H, W)\n",
    "  - cache: Values from the forward pass\n",
    "  \n",
    "  Returns a tuple of:\n",
    "  - dx: Gradient with respect to inputs, of shape (N, C, H, W)\n",
    "  - dgamma: Gradient with respect to scale parameter, of shape (C,)\n",
    "  - dbeta: Gradient with respect to shift parameter, of shape (C,)\n",
    "  \"\"\"\n",
    "  dx, dgamma, dbeta = None, None, None\n",
    "\n",
    "  #############################################################################\n",
    "  # TODO: Implement the backward pass for spatial batch normalization.        #\n",
    "  #                                                                           #\n",
    "  # HINT: You can implement spatial batch normalization using the vanilla     #\n",
    "  # version of batch normalization defined above. Your implementation should  #\n",
    "  # be very short; ours is less than five lines.                              #\n",
    "  #############################################################################\n",
    "  pass\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "\n",
    "  return dx, dgamma, dbeta\n",
    "  \n",
    "\n",
    "def svm_loss(x, y):\n",
    "  \"\"\"\n",
    "  Computes the loss and gradient using for multiclass SVM classification.\n",
    "\n",
    "  Inputs:\n",
    "  - x: Input data, of shape (N, C) where x[i, j] is the score for the jth class\n",
    "    for the ith input.\n",
    "  - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "    0 <= y[i] < C\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - loss: Scalar giving the loss\n",
    "  - dx: Gradient of the loss with respect to x\n",
    "  \"\"\"\n",
    "  N = x.shape[0]\n",
    "  correct_class_scores = x[np.arange(N), y]\n",
    "  margins = np.maximum(0, x - correct_class_scores[:, np.newaxis] + 1.0)\n",
    "  margins[np.arange(N), y] = 0\n",
    "  loss = np.sum(margins) / N\n",
    "  num_pos = np.sum(margins > 0, axis=1)\n",
    "  dx = np.zeros_like(x)\n",
    "  dx[margins > 0] = 1\n",
    "  dx[np.arange(N), y] -= num_pos\n",
    "  dx /= N\n",
    "  return loss, dx\n",
    "\n",
    "\n",
    "def softmax_loss(x, y):\n",
    "  \"\"\"\n",
    "  Computes the loss and gradient for softmax classification.\n",
    "\n",
    "  Inputs:\n",
    "  - x: Input data, of shape (N, C) where x[i, j] is the score for the jth class\n",
    "    for the ith input.\n",
    "  - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "    0 <= y[i] < C\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - loss: Scalar giving the loss\n",
    "  - dx: Gradient of the loss with respect to x\n",
    "  \"\"\"\n",
    "  probs = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "  probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "  N = x.shape[0]\n",
    "  loss = -np.sum(np.log(probs[np.arange(N), y])) / N\n",
    "  dx = probs.copy()\n",
    "  dx[np.arange(N), y] -= 1\n",
    "  dx /= N\n",
    "  return loss, dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

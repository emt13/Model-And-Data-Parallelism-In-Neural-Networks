{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fc_net.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fc_net.py\n",
    "# %load fc_net.py\n",
    "import numpy as np\n",
    "\n",
    "from deeplearning.layers import *\n",
    "from deeplearning.layer_utils import *\n",
    "\n",
    "\n",
    "class TwoLayerNet(object):\n",
    "  \"\"\"\n",
    "  A two-layer fully-connected neural network with ReLU nonlinearity and\n",
    "  softmax loss that uses a modular layer design. We assume an input dimension\n",
    "  of D, a hidden dimension of H, and perform classification over C classes.\n",
    "  \n",
    "  The architecure should be affine - relu - affine - softmax.\n",
    "\n",
    "  Note that this class does not implement gradient descent; instead, it\n",
    "  will interact with a separate Solver object that is responsible for running\n",
    "  optimization.\n",
    "\n",
    "  The learnable parameters of the model are stored in the dictionary\n",
    "  self.params that maps parameter names to numpy arrays.\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, input_dim=3*32*32, hidden_dim=100, num_classes=10,\n",
    "               weight_scale=1e-3, reg=0.0):\n",
    "    \"\"\"\n",
    "    Initialize a new network.\n",
    "\n",
    "    Inputs:\n",
    "    - input_dim: An integer giving the size of the input\n",
    "    - hidden_dim: An integer giving the size of the hidden layer\n",
    "    - num_classes: An integer giving the number of classes to classify\n",
    "    - dropout: Scalar between 0 and 1 giving dropout strength.\n",
    "    - weight_scale: Scalar giving the standard deviation for random\n",
    "      initialization of the weights.\n",
    "    - reg: Scalar giving L2 regularization strength.\n",
    "    \"\"\"\n",
    "    self.params = {}\n",
    "    self.reg = reg\n",
    "    \n",
    "    ############################################################################\n",
    "    # TODO: Initialize the weights and biases of the two-layer net. Weights    #\n",
    "    # should be initialized from a Gaussian with standard deviation equal to   #\n",
    "    # weight_scale, and biases should be initialized to zero. All weights and  #\n",
    "    # biases should be stored in the dictionary self.params, with first layer  #\n",
    "    # weights and biases using the keys 'W1' and 'b1' and second layer weights #\n",
    "    # and biases using the keys 'W2' and 'b2'.                                 #\n",
    "    ############################################################################\n",
    "    # weights dim: D x H\n",
    "    # bias dim: H x 1\n",
    "    self.params['W1'] = np.random.normal(scale = weight_scale, size=(input_dim, hidden_dim))\n",
    "    self.params['W2'] = np.random.normal(scale = weight_scale, size=(hidden_dim, num_classes))\n",
    "    self.params['b1'] = np.zeros(hidden_dim)\n",
    "    self.params['b2'] = np.zeros(num_classes)\n",
    "    ############################################################################\n",
    "    #                             END OF YOUR CODE                             #\n",
    "    ############################################################################\n",
    "\n",
    "\n",
    "  def loss(self, X, y=None):\n",
    "    \"\"\"\n",
    "    Compute loss and gradient for a minibatch of data.\n",
    "\n",
    "    Inputs:\n",
    "    - X: Array of input data of shape (N, d_1, ..., d_k)\n",
    "    - y: Array of labels, of shape (N,). y[i] gives the label for X[i].\n",
    "\n",
    "    Returns:\n",
    "    If y is None, then run a test-time forward pass of the model and return:\n",
    "    - scores: Array of shape (N, C) giving classification scores, where\n",
    "      scores[i, c] is the classification score for X[i] and class c.\n",
    "\n",
    "    If y is not None, then run a training-time forward and backward pass and\n",
    "    return a tuple of:\n",
    "    - loss: Scalar value giving the loss\n",
    "    - grads: Dictionary with the same keys as self.params, mapping parameter\n",
    "      names to gradients of the loss with respect to those parameters.\n",
    "    \"\"\"  \n",
    "    scores = None\n",
    "    ############################################################################\n",
    "    # TODO: Implement the forward pass for the two-layer net, computing the    #\n",
    "    # class scores for X and storing them in the scores variable.              #\n",
    "    ############################################################################\n",
    "    \n",
    "    hidden_layer, cache_first = affine_relu_forward(X, self.params['W1'], self.params['b1'])\n",
    "    # scores = np.dot(hidden_layer, self.params['W2']) + self.params['b2']\n",
    "    # print(\"Wasss\")\n",
    "    # print(hidden_layer.shape)\n",
    "    scores, cache_second = affine_forward(hidden_layer, self.params['W2'], self.params['b2'])\n",
    "    \n",
    "    ############################################################################\n",
    "    #                             END OF YOUR CODE                             #\n",
    "    ############################################################################\n",
    "\n",
    "    # If y is None then we are in test mode so just return scores\n",
    "    if y is None:\n",
    "      return scores\n",
    "    \n",
    "    loss, grads = 0, {}\n",
    "    ############################################################################\n",
    "    # TODO: Implement the backward pass for the two-layer net. Store the loss  #\n",
    "    # in the loss variable and gradients in the grads dictionary. Compute data #\n",
    "    # loss using softmax, and make sure that grads[k] holds the gradients for  #\n",
    "    # self.params[k]. Don't forget to add L2 regularization!                   #\n",
    "    #                                                                          #\n",
    "    # NOTE: To ensure that your implementation matches ours and you pass the   #\n",
    "    # automated tests, make sure that your L2 regularization includes a factor #\n",
    "    # of 0.5 to simplify the expression for the gradient.                      #\n",
    "    ############################################################################\n",
    "    data_loss, dx = softmax_loss(scores, y)\n",
    "    loss = data_loss + 0.5 * self.reg * np.sum(self.params['W1'] * self.params['W1']) + 0.5 * self.reg * np.sum(self.params['W2'] * self.params['W2'])\n",
    "    \n",
    "    second_grads = affine_backward(dx, cache_second)\n",
    "    grads['W2'] = second_grads[1] + self.reg * self.params['W2']# dw\n",
    "    grads['b2'] = second_grads[2] # db\n",
    "    first_grads = affine_relu_backward(second_grads[0], cache_first) #dx\n",
    "    grads['W1'] = first_grads[1] + self.reg * self.params['W1']\n",
    "    grads['b1'] = first_grads[2]\n",
    "    \n",
    "    # print()\n",
    "    \n",
    "    ############################################################################\n",
    "    #                             END OF YOUR CODE                             #\n",
    "    ############################################################################\n",
    "\n",
    "    return loss, grads\n",
    "\n",
    "\n",
    "class FullyConnectedNet(object):\n",
    "  \"\"\"\n",
    "  A fully-connected neural network with an arbitrary number of hidden layers,\n",
    "  ReLU nonlinearities, and a softmax loss function. This will also implement\n",
    "  dropout and batch normalization as options. For a network with L layers,\n",
    "  the architecture will be\n",
    "  \n",
    "  {affine - [batch norm] - relu - [dropout]} x (L - 1) - affine - softmax\n",
    "  \n",
    "  where batch normalization and dropout are optional, and the {...} block is\n",
    "  repeated L - 1 times.\n",
    "  \n",
    "  Similar to the TwoLayerNet above, learnable parameters are stored in the\n",
    "  self.params dictionary and will be learned using the Solver class.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, hidden_dims, input_dim=3*32*32, num_classes=10,\n",
    "               dropout=0, use_batchnorm=False, reg=0.0,\n",
    "               weight_scale=1e-2, dtype=np.float32, seed=None):\n",
    "    \"\"\"\n",
    "    Initialize a new FullyConnectedNet.\n",
    "    \n",
    "    Inputs:\n",
    "    - hidden_dims: A list of integers giving the size of each hidden layer.\n",
    "    - input_dim: An integer giving the size of the input.\n",
    "    - num_classes: An integer giving the number of classes to classify.\n",
    "    - dropout: Scalar between 0 and 1 giving dropout strength. If dropout=0 then\n",
    "      the network should not use dropout at all.\n",
    "    - use_batchnorm: Whether or not the network should use batch normalization.\n",
    "    - reg: Scalar giving L2 regularization strength.\n",
    "    - weight_scale: Scalar giving the standard deviation for random\n",
    "      initialization of the weights.\n",
    "    - dtype: A numpy datatype object; all computations will be performed using\n",
    "      this datatype. float32 is faster but less accurate, so you should use\n",
    "      float64 for numeric gradient checking.\n",
    "    - seed: If not None, then pass this random seed to the dropout layers. This\n",
    "      will make the dropout layers deteriminstic so we can gradient check the\n",
    "      model.\n",
    "    \"\"\"\n",
    "    self.use_batchnorm = use_batchnorm\n",
    "    self.use_dropout = dropout > 0\n",
    "    self.reg = reg\n",
    "    self.num_layers = 1 + len(hidden_dims)\n",
    "    self.dtype = dtype\n",
    "    self.params = {}\n",
    "\n",
    "    ############################################################################\n",
    "    # TODO: Initialize the parameters of the network, storing all values in    #\n",
    "    # the self.params dictionary. Store weights and biases for the first layer #\n",
    "    # in W1 and b1; for the second layer use W2 and b2, etc. Weights should be #\n",
    "    # initialized from a normal distribution with standard deviation equal to  #\n",
    "    # weight_scale and biases should be initialized to zero.                   #\n",
    "    #                                                                          #\n",
    "    # When using batch normalization, store scale and shift parameters for the #\n",
    "    # first layer in gamma1 and beta1; for the second layer use gamma2 and     #\n",
    "    # beta2, etc. Scale parameters should be initialized to one and shift      #\n",
    "    # parameters should be initialized to zero.                                #\n",
    "    ############################################################################\n",
    "    \n",
    "    dims = [input_dim] + hidden_dims + [num_classes]\n",
    "    \n",
    "    for i in range(self.num_layers):\n",
    "        W = 'W' + str(i+1)\n",
    "        b = 'b' + str(i+1)\n",
    "        self.params[W] = np.random.normal(scale = weight_scale, size=(dims[i], dims[i+1]))\n",
    "        self.params[b] = np.zeros(dims[i+1])\n",
    "   \n",
    "    \n",
    "    ############################################################################\n",
    "    #                             END OF YOUR CODE                             #\n",
    "    ############################################################################\n",
    "\n",
    "    # When using dropout we need to pass a dropout_param dictionary to each\n",
    "    # dropout layer so that the layer knows the dropout probability and the mode\n",
    "    # (train / test). You can pass the same dropout_param to each dropout layer.\n",
    "    self.dropout_param = {}\n",
    "    if self.use_dropout:\n",
    "      self.dropout_param = {'mode': 'train', 'p': dropout}\n",
    "      if seed is not None:\n",
    "        self.dropout_param['seed'] = seed\n",
    "    \n",
    "    # With batch normalization we need to keep track of running means and\n",
    "    # variances, so we need to pass a special bn_param object to each batch\n",
    "    # normalization layer. You should pass self.bn_params[0] to the forward pass\n",
    "    # of the first batch normalization layer, self.bn_params[1] to the forward\n",
    "    # pass of the second batch normalization layer, etc.\n",
    "    self.bn_params = []\n",
    "    if self.use_batchnorm:\n",
    "      self.bn_params = [{'mode': 'train'} for i in xrange(self.num_layers - 1)]\n",
    "    \n",
    "    # Cast all parameters to the correct datatype\n",
    "    for k, v in self.params.iteritems():\n",
    "      self.params[k] = v.astype(dtype)\n",
    "\n",
    "\n",
    "  def loss(self, X, y=None):\n",
    "    \"\"\"\n",
    "    Compute loss and gradient for the fully-connected net.\n",
    "\n",
    "    Input / output: Same as TwoLayerNet above.\n",
    "    \"\"\"\n",
    "    X = X.astype(self.dtype)\n",
    "    mode = 'test' if y is None else 'train'\n",
    "\n",
    "    # Set train/test mode for batchnorm params and dropout param since they\n",
    "    # behave differently during training and testing.\n",
    "    if self.dropout_param is not None:\n",
    "      self.dropout_param['mode'] = mode   \n",
    "    if self.use_batchnorm:\n",
    "      for bn_param in self.bn_params:\n",
    "        bn_param[mode] = mode\n",
    "\n",
    "    scores = None\n",
    "    ############################################################################\n",
    "    # TODO: Implement the forward pass for the fully-connected net, computing  #\n",
    "    # the class scores for X and storing them in the scores variable.          #\n",
    "    #                                                                          #\n",
    "    # When using dropout, you'll need to pass self.dropout_param to each       #\n",
    "    # dropout forward pass.                                                    #\n",
    "    #                                                                          #\n",
    "    # When using batch normalization, you'll need to pass self.bn_params[0] to #\n",
    "    # the forward pass for the first batch normalization layer, pass           #\n",
    "    # self.bn_params[1] to the forward pass for the second batch normalization #\n",
    "    # layer, etc.                                                              #\n",
    "    ############################################################################\n",
    "    \n",
    "    input_layer = X\n",
    "    cache = {}\n",
    "    \n",
    "    for i in range(self.num_layers - 1):\n",
    "        W = 'W' + str(i+1)\n",
    "        b = 'b' + str(i+1)\n",
    "        input_layer, cache[str(i+1)] = affine_relu_forward(input_layer, self.params[W], self.params[b])\n",
    "    \n",
    "    scores, cache[str(self.num_layers)] = affine_forward(input_layer, self.params['W' + str(self.num_layers)], self.params['b' + str(self.num_layers)])\n",
    "    \n",
    "    \n",
    "    ############################################################################\n",
    "    #                             END OF YOUR CODE                             #\n",
    "    ############################################################################\n",
    "\n",
    "    # If test mode return early\n",
    "    if mode == 'test':\n",
    "      return scores\n",
    "\n",
    "    loss, grads = 0.0, {}\n",
    "    ############################################################################\n",
    "    # TODO: Implement the backward pass for the fully-connected net. Store the #\n",
    "    # loss in the loss variable and gradients in the grads dictionary. Compute #\n",
    "    # data loss using softmax, and make sure that grads[k] holds the gradients #\n",
    "    # for self.params[k]. Don't forget to add L2 regularization!               #\n",
    "    #                                                                          #\n",
    "    # When using batch normalization, you don't need to regularize the scale   #\n",
    "    # and shift parameters.                                                    #\n",
    "    #                                                                          #\n",
    "    # NOTE: To ensure that your implementation matches ours and you pass the   #\n",
    "    # automated tests, make sure that your L2 regularization includes a factor #\n",
    "    # of 0.5 to simplify the expression for the gradient.                      #\n",
    "    ############################################################################\n",
    "    loss, derv = softmax_loss(scores, y)\n",
    "\n",
    "    # Lysia\n",
    "    for i in range(self.num_layers, 0, -1):\n",
    "        W = 'W' + str(i)\n",
    "        b = 'b' + str(i)\n",
    "        loss += 0.5 * self.reg * np.sum(self.params[W] ** 2)\n",
    "        \n",
    "        if(i == self.num_layers):\n",
    "            derv, grads[W], grads[b] = affine_backward(derv, cache[str(i)])\n",
    "        else:\n",
    "            derv, grads[W], grads[b] = affine_relu_backward(derv, cache[str(i)])\n",
    "        \n",
    "        grads[W] += self.reg * self.params[W]\n",
    "            \n",
    "        \n",
    "    \n",
    "    ############################################################################\n",
    "    #                             END OF YOUR CODE                             #\n",
    "    ############################################################################\n",
    "\n",
    "    return loss, grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOT RELEVANT BELOW THIS LINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __init__(self, input_dim=3*32*32, hidden_dim=100, num_classes=10,\n",
    "               weight_scale=1e-3, reg=0.0):\n",
    "    \"\"\"\n",
    "    Initialize a new network.\n",
    "\n",
    "    Inputs:\n",
    "    - input_dim: An integer giving the size of the input\n",
    "    - hidden_dim: An integer giving the size of the hidden layer\n",
    "    - num_classes: An integer giving the number of classes to classify\n",
    "    - dropout: Scalar between 0 and 1 giving dropout strength.\n",
    "    - weight_scale: Scalar giving the standard deviation for random\n",
    "      initialization of the weights.\n",
    "    - reg: Scalar giving L2 regularization strength.\n",
    "    \"\"\"\n",
    "    self.params = {}\n",
    "    self.reg = reg\n",
    "    \n",
    "    ############################################################################\n",
    "    # TODO: Initialize the weights and biases of the two-layer net. Weights    #\n",
    "    # should be initialized from a Gaussian with standard deviation equal to   #\n",
    "    # weight_scale, and biases should be initialized to zero. All weights and  #\n",
    "    # biases should be stored in the dictionary self.params, with first layer  #\n",
    "    # weights and biases using the keys 'W1' and 'b1' and second layer weights #\n",
    "    # and biases using the keys 'W2' and 'b2'.                                 #\n",
    "    ############################################################################\n",
    "    \n",
    "    # weights dim: D x H\n",
    "    # bias dim: H x 1\n",
    "    \n",
    "    self.params['W1'] = np.random.normal(scale = weight_scale, size=(input_dim, hidden_dim))\n",
    "    self.params['W2'] = np.random.normal(scale = weight_scale, size=(input_dim, hidden_dim))\n",
    "    self.params['b1'] = np.zeros((hidden_dim,1))\n",
    "    self.params['b2'] = np.zeros((hidden_dim,1))\n",
    "    \n",
    "    ############################################################################\n",
    "    #                             END OF YOUR CODE                             #\n",
    "    ############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(self, X, y=None):\n",
    "    \"\"\"\n",
    "    Compute loss and gradient for a minibatch of data.\n",
    "\n",
    "    Inputs:\n",
    "    - X: Array of input data of shape (N, d_1, ..., d_k)\n",
    "    - y: Array of labels, of shape (N,). y[i] gives the label for X[i].\n",
    "\n",
    "    Returns:\n",
    "    If y is None, then run a test-time forward pass of the model and return:\n",
    "    - scores: Array of shape (N, C) giving classification scores, where\n",
    "      scores[i, c] is the classification score for X[i] and class c.\n",
    "\n",
    "    If y is not None, then run a training-time forward and backward pass and\n",
    "    return a tuple of:\n",
    "    - loss: Scalar value giving the loss\n",
    "    - grads: Dictionary with the same keys as self.params, mapping parameter\n",
    "      names to gradients of the loss with respect to those parameters.\n",
    "    \"\"\"  \n",
    "    scores = None\n",
    "    ############################################################################\n",
    "    # TODO: Implement the forward pass for the two-layer net, computing the    #\n",
    "    # class scores for X and storing them in the scores variable.              #\n",
    "    ############################################################################\n",
    "    \n",
    "    hidden_layer, cache = affline_relu_forward(X, self.params['W1'], self.params['b1'])\n",
    "    scores = np.dot(hidden_layer, self.params['W2']) + self.params['b2']\n",
    "    \n",
    "    ############################################################################\n",
    "    #                             END OF YOUR CODE                             #\n",
    "    ############################################################################\n",
    "\n",
    "    # If y is None then we are in test mode so just return scores\n",
    "    if y is None:\n",
    "      return scores\n",
    "    \n",
    "    loss, grads = 0, {}\n",
    "    ############################################################################\n",
    "    # TODO: Implement the backward pass for the two-layer net. Store the loss  #\n",
    "    # in the loss variable and gradients in the grads dictionary. Compute data #\n",
    "    # loss using softmax, and make sure that grads[k] holds the gradients for  #\n",
    "    # self.params[k]. Don't forget to add L2 regularization!                   #\n",
    "    #                                                                          #\n",
    "    # NOTE: To ensure that your implementation matches ours and you pass the   #\n",
    "    # automated tests, make sure that your L2 regularization includes a factor #\n",
    "    # of 0.5 to simplify the expression for the gradient.                      #\n",
    "    ############################################################################\n",
    "    pass\n",
    "    ############################################################################\n",
    "    #                             END OF YOUR CODE                             #\n",
    "    ############################################################################\n",
    "\n",
    "    return loss, grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0\n",
      "W1\n",
      "W2\n",
      "W3\n",
      "W4\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    tmp = \"W\" + str(i)\n",
    "    print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

## Entry files to look at

(Note: I only implemented Fully Connected Networks. I did not implement CNN (you can refer to Avery's).)

- **FullyConnectedNets.ipynb**: main file that runs everything

- **Layers.ipynb**: the modular functions for NN, i.e affine\_forward, affine\_backward, relu\_forward, etc

- **deeplearning/classifiers/fc_net.ipynb**: the actual FullyConnectedNet class

- **deeplearning/Solver.ipynb**: a Solver encapsulates all the logic necessary for training classification
  models.
  
## Stanford resources 

- [http://cs231n.github.io/optimization-2/] explains backprop very well. It also gives code examples performing forward pass and back prop using matrix multiplication

- [http://cs231n.github.io/] has all other course notes, including SGD, loss, etc.
